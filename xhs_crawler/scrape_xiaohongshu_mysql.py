# -*- coding: utf-8 -*-
import os
import time
import re
import yaml
from datetime import datetime, timedelta
import urllib.parse
import pymysql
from playwright.sync_api import sync_playwright, TimeoutError
import requests
import random
import uuid

# ===========================
# ğŸ§© è¯»å–é…ç½®æ–‡ä»¶
# ===========================
with open("config.yaml", "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

MYSQL_DB_HOST = config["mysql"]["host"]
MYSQL_DB_PORT = int(config["mysql"]["port"])
MYSQL_DB_USER = config["mysql"]["user"]
MYSQL_DB_PWD = config["mysql"]["password"]
MYSQL_DB_NAME = config["mysql"]["database"]

KEYWORDS = [kw.strip() for kw in config["crawler"]["keywords"].split(",")]
MAX_NOTES_PER_KEYWORD = int(config["crawler"].get("max_notes_per_keyword", 30))
MAX_COMMENTS_PER_NOTE = int(config["crawler"].get("max_comments_per_note", -1))
SCROLL_PAUSE = float(config["crawler"].get("scroll_pause", 2.0))
SCROLL_RETRY = int(config["crawler"].get("scroll_retry", 3))
RANDOM_DELAY_MIN = float(config["crawler"].get("random_delay_min", 1.5))
RANDOM_DELAY_MAX = float(config["crawler"].get("random_delay_max", 4.0))
MAX_RETRIES = int(config["crawler"].get("max_retries", 3))
BATCH_SLEEP_AFTER = int(config["crawler"].get("batch_sleep_after", 10))
BATCH_SLEEP_SEC = int(config["crawler"].get("batch_sleep_sec", 300))
USER_AGENT_ROTATE = bool(config["crawler"].get("user_agent_rotate", True))
PROXY = config["crawler"].get("proxy", "")

# ===========================
# ğŸ§© æ•°æ®åº“æ“ä½œ
# ===========================
def get_conn():
    return pymysql.connect(
        host=MYSQL_DB_HOST,
        port=MYSQL_DB_PORT,
        user=MYSQL_DB_USER,
        password=MYSQL_DB_PWD,
        database=MYSQL_DB_NAME,
        charset="utf8mb4"
    )

def init_db():
    conn = get_conn()
    cur = conn.cursor()
    # ç¬”è®°è¡¨
    cur.execute("""
        CREATE TABLE IF NOT EXISTS xhs_notes (
            id BIGINT AUTO_INCREMENT PRIMARY KEY,
            note_id VARCHAR(64) NOT NULL,
            title VARCHAR(255),
            node_text TEXT,
            author VARCHAR(255),
            user_id VARCHAR(64),
            publish_time VARCHAR(100),
            url TEXT,
            keywords TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            -- âœ… å”¯ä¸€çº¦æŸï¼šnote_id å”¯ä¸€
            UNIQUE KEY uk_note_id (note_id),
            INDEX idx_author (author),
            INDEX idx_user_id (user_id)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    """)

    # è¯„è®ºè¡¨
    cur.execute("""
        CREATE TABLE IF NOT EXISTS xhs_comments (
            id VARCHAR(255) PRIMARY KEY,
            note_id VARCHAR(64) NOT NULL,
            user_id VARCHAR(64),
            user_name VARCHAR(255),
            user_url TEXT,
            location VARCHAR(255),
            content TEXT,
            comment_time VARCHAR(100),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            INDEX idx_note_id (note_id),
            INDEX idx_user_id (user_id),
            -- âœ… è”åˆå”¯ä¸€çº¦æŸï¼šåŒä¸€ note_id + content(255) + user_id ä¸é‡å¤
            UNIQUE KEY uk_note_comment_user (note_id, content(255), user_id),
            FOREIGN KEY (note_id) REFERENCES xhs_notes(note_id) ON DELETE CASCADE
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    """)

    # ç”¨æˆ·è¡¨
    cur.execute("""
        CREATE TABLE IF NOT EXISTS xhs_users (
            id BIGINT AUTO_INCREMENT PRIMARY KEY,
            user_id VARCHAR(64) UNIQUE,
            user_url TEXT,
            user_name VARCHAR(255),
            user_red_id VARCHAR(255),
            location VARCHAR(255),
            gender VARCHAR(10),
            avatar_url TEXT,
            followers VARCHAR(50),
            following VARCHAR(50),
            likes VARCHAR(50),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            INDEX idx_user_id (user_id)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
    """)
    conn.commit()
    cur.close()
    conn.close()
    print("âœ… æ•°æ®åº“ç»“æ„å·²åˆå§‹åŒ–ï¼ˆxhs_notes + xhs_comments + xhs_usersï¼‰")

# ===========================
# ğŸ§© å·¥å…·å‡½æ•°ï¼ˆæ—¶é—´ã€IDè§£æï¼‰
# ===========================
def extract_id_from_url(url: str) -> str | None:
    if url is None or url == "":
        return None
    match = re.search(r'/([^/?]+)(?:\?|$)', url)
    return match.group(1) if match else None
def parse_xiaohongshu_time(time_str: str, now: datetime = None) -> str | None:
    """
    è§£æå°çº¢ä¹¦å„ç§æ—¶é—´æ˜¾ç¤ºæ ¼å¼ï¼Œç»Ÿä¸€è½¬ä¸º 'YYYY-MM-DD'ï¼ˆæˆ–æ›´ç²¾ç¡®åˆ°ç§’ï¼ŒæŒ‰éœ€æ‰©å±•ï¼‰ã€‚
    
    æ”¯æŒæ ¼å¼ï¼š
      - "åˆšåˆš"
      - "3åˆ†é’Ÿå‰"
      - "2å°æ—¶å‰"
      - "ä»Šå¤© 14:30"
      - "æ˜¨å¤© 09:15"
      - "4å¤©å‰"
      - "10-12"
      - "2024-03-15"
      - "2025-02-14 13:45:22" ï¼ˆå…¼å®¹ï¼‰
    
    Args:
        time_str: åŸå§‹æ—¶é—´å­—ç¬¦ä¸²
        now: å‚è€ƒæ—¶é—´ï¼ˆé»˜è®¤ä¸ºå½“å‰ç³»ç»Ÿæ—¶é—´ï¼‰
    
    Returns:
        æ ‡å‡† ISO æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆå¦‚ "2025-11-07"ï¼‰ï¼Œæˆ– None
    """
    if now is None:
        now = datetime.now()
    
    time_str = time_str.strip()
    if not time_str or time_str in ("N/A", "æ— ", "æœªçŸ¥"):
        return None

    # 1. åˆšåˆš
    if "åˆšåˆš" in time_str:
        return now.strftime("%Y-%m-%d")

    # 2. Xåˆ†é’Ÿå‰
    min_match = re.match(r'^(\d+)åˆ†é’Ÿå‰$', time_str)
    if min_match:
        minutes = int(min_match.group(1))
        dt = now - timedelta(minutes=minutes)
        return dt.strftime("%Y-%m-%d")

    # 3. Xå°æ—¶å‰
    hour_match = re.match(r'^(\d+)å°æ—¶å‰$', time_str)
    if hour_match:
        hours = int(hour_match.group(1))
        dt = now - timedelta(hours=hours)
        return dt.strftime("%Y-%m-%d")

    # 4. ä»Šå¤© HH:mm[:ss]
    today_match = re.match(r'^ä»Šå¤©\s+(\d{1,2}):(\d{2})(?::(\d{2}))?$', time_str)
    if today_match:
        h, m, s = int(today_match.group(1)), int(today_match.group(2)), today_match.group(3)
        s = int(s) if s else 0
        dt = now.replace(hour=h, minute=m, second=s, microsecond=0)
        return dt.strftime("%Y-%m-%d")

    # 5. æ˜¨å¤© HH:mm[:ss]
    yesterday_match = re.match(r'^æ˜¨å¤©\s+(\d{1,2}):(\d{2})(?::(\d{2}))?$', time_str)
    if yesterday_match:
        h, m, s = int(yesterday_match.group(1)), int(yesterday_match.group(2)), yesterday_match.group(3)
        s = int(s) if s else 0
        dt = (now - timedelta(days=1)).replace(hour=h, minute=m, second=s, microsecond=0)
        return dt.strftime("%Y-%m-%d")

    # 6. Xå¤©å‰
    day_match = re.match(r'^(\d+)å¤©å‰$', time_str)
    if day_match:
        days = int(day_match.group(1))
        dt = now - timedelta(days=days)
        return dt.strftime("%Y-%m-%d")

    # 7. YYYY-MM-DD HH:mm:ss ï¼ˆå®Œæ•´æ—¶é—´ï¼‰
    full_match = re.match(r'^(\d{4})-(\d{1,2})-(\d{1,2})\s+(\d{1,2}):(\d{2})(?::(\d{2}))?$', time_str)
    if full_match:
        y, mo, d, h, m, s = map(int, full_match.groups()[:5]) + (int(full_match.group(6)) if full_match.group(6) else 0,)
        try:
            dt = datetime(y, mo, d, h, m, s)
            return dt.strftime("%Y-%m-%d")
        except ValueError:
            pass

    # 8. YYYY-MM-DD
    ymd_match = re.match(r'^(\d{4})-(\d{1,2})-(\d{1,2})$', time_str)
    if ymd_match:
        y, mo, d = map(int, ymd_match.groups())
        try:
            dt = datetime(y, mo, d)
            return dt.strftime("%Y-%m-%d")
        except ValueError:
            pass

    # 9. MM-DD ï¼ˆæœ€å¸¸è§äºæœç´¢é¡µï¼‰
    md_match = re.match(r'^(\d{1,2})-(\d{1,2})$', time_str)
    if md_match:
        mo, d = map(int, md_match.groups())
        try:
            # å…ˆè¯•ä»Šå¹´
            candidate = datetime(now.year, mo, d)
            if candidate > now:
                candidate = datetime(now.year - 1, mo, d)
            return candidate.strftime("%Y-%m-%d")
        except ValueError:
            pass

    # æ— æ³•è¯†åˆ«
    return time_str

def random_wait(base=RANDOM_DELAY_MIN, var=RANDOM_DELAY_MAX):
    delay = base + random.random() * (var - base)
    print(f"â³ éšæœºç­‰å¾… {delay:.2f} ç§’...")
    time.sleep(delay)

def with_retry(max_retries=MAX_RETRIES):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for i in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"âš ï¸ {func.__name__} ç¬¬ {i+1} æ¬¡å¤±è´¥ï¼š{e}")
                    time.sleep(3 + i * 2)
            print(f"âŒ {func.__name__} å¤šæ¬¡å¤±è´¥ï¼Œè·³è¿‡ã€‚")
            return None
        return wrapper
    return decorator

# ===========================
# ğŸ§© æ•°æ®ä¿å­˜å‡½æ•°
# ===========================
def save_note_to_db(note):
    conn = get_conn()
    cur = conn.cursor()
    try:
        cur.execute("""
            INSERT IGNORE INTO xhs_notes (note_id, title, node_text, author, user_id, publish_time, url, keywords)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """, (note["note_id"], note["title"], note["node_text"], note["author"], note.get("user_id"), note["time"], note["url"], note["keyword"]))
        conn.commit()
    except Exception as e:
        print("âŒ ä¿å­˜ç¬”è®°å¤±è´¥:", e)
    finally:
        cur.close()
        conn.close()

def save_comments_to_db(note_url, comments):
    conn = get_conn()
    cur = conn.cursor()
    note_id = extract_id_from_url(note_url)
    if not note_id:
        print("âš ï¸ æ— æ³•æå–ç¬”è®°IDï¼Œè·³è¿‡è¯„è®ºä¿å­˜")
        return
    try:
        ids = []
        for cmt in comments:
            id = str(uuid.uuid4())
            cur.execute("""
                INSERT INTO xhs_comments (id, note_id, user_name, content, comment_time, user_id, user_url, location)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (id, note_id, cmt["user"], cmt["content"], cmt["time"], cmt.get("user_id"), cmt.get("user_url"), cmt.get("location")))
            ids.append(id)
        conn.commit()
        for id in ids:
            requests.post("http://127.0.0.1:5000/api/embeddings", json={
                "comment_id": id
            })
        print(f"ğŸ’¾ å·²æ–°å¢ {len(comments)} æ¡è¯„è®º")
    except Exception as e:
        print("âŒ ä¿å­˜è¯„è®ºå¤±è´¥:", e)
    finally:
        cur.close()
        conn.close()

# ===========================
# ğŸ§© ç”¨æˆ·æ£€æŸ¥ä¸ä¿å­˜
# ===========================
def user_exists(user_id, user_url):
    conn = get_conn()
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM xhs_users WHERE user_id=%s OR user_url=%s", (user_id, user_url))
    exists = cur.fetchone()[0] > 0
    cur.close()
    conn.close()
    return exists

def save_user_to_db(user):
    conn = get_conn()
    cur = conn.cursor()
    try:
        cur.execute("""
            INSERT IGNORE INTO xhs_users (user_id, user_url, user_name, user_red_id, location, gender, avatar_url, followers, following, likes)
            VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
        """, (
            user.get("user_id"), user.get("user_url"), user.get("user_name"), user.get("user_red_id"),
            user.get("location"), user.get("gender"), user.get("avatar_url"),
            user.get("followers"), user.get("following"), user.get("likes")
        ))
        conn.commit()
        print(f"âœ… ç”¨æˆ· {user.get('user_name')} ä¿¡æ¯å·²ä¿å­˜")
    except Exception as e:
        print("âŒ ä¿å­˜ç”¨æˆ·å¤±è´¥:", e)
    finally:
        cur.close()
        conn.close()

# ===========================
# ğŸ§© çˆ¬å–ç”¨æˆ·è¯¦æƒ…
# ===========================
        
@with_retry()
def scrape_user_detail(context, user_url):
    if not user_url:
        return None

    user_id = extract_id_from_url(user_url)
    if user_exists(user_id, user_url):
        print(f"â­ï¸ ç”¨æˆ· {user_id} å·²å­˜åœ¨ï¼Œè·³è¿‡")
        return None

    print(f"ğŸ§­ æ­£åœ¨çˆ¬å–ç”¨æˆ·è¯¦æƒ…: {user_url}")
    page = context.new_page()
    try:
        page.goto(f"https://www.xiaohongshu.com{user_url}", wait_until="domcontentloaded", timeout=20000)
        page.wait_for_selector(".info", timeout=15000)

        user_name = page.locator(".user-name").inner_text(timeout=5000)
        red_id = page.locator(".user-redId").inner_text(timeout=5000).replace("å°çº¢ä¹¦å·ï¼š", "").strip() if page.locator(".user-redId").count() else ""
        location = page.locator(".user-IP").inner_text(timeout=5000).replace("IPå±åœ°ï¼š", "").strip() if page.locator(".user-IP").count() else ""
        #avatar_url = page.locator(".avatar img").get_attribute("src") if page.locator(".avatar img").count() else ""
        gender = "å¥³" if page.locator(".gender use[xlink\\:href='#female']").count() else ("ç”·" if page.locator(".gender use[xlink\\:href='#male']").count() else "")
        counts = page.locator(".user-interactions div span.count").all_inner_texts()
        following, followers, likes = (counts + ["", "", ""])[:3]

        user_data = {
            "user_id": user_id,
            "user_url": user_url,
            "user_name": user_name,
            "user_red_id": red_id,
            "location": location,
            "gender": gender,
            #"avatar_url": avatar_url,
            "followers": followers,
            "following": following,
            "likes": likes,
        }

        save_user_to_db(user_data)
        print(f"âœ… ç”¨æˆ·è¯¦æƒ…æŠ“å–å®Œæˆ: {user_name} ({user_id})")
        return user_data
    except Exception as e:
        print("âŒ ç”¨æˆ·è¯¦æƒ…æŠ“å–å¤±è´¥:", e)
        return None
    finally:
        page.close()

# ===========================
# ğŸ§© çˆ¬å–ç¬”è®°ä¸è¯„è®º
# ===========================
def scrape_keyword(context,page, keyword):
    encoded_keyword = urllib.parse.quote(keyword)
    target_url = f"https://www.xiaohongshu.com/search_result?keyword={encoded_keyword}"
    print(f"\nğŸ” æ­£åœ¨æœç´¢: {keyword}")

    try:
        page.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        window.navigator.chrome = { runtime: {} };
        Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh'] });
        Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
        """)
        page.goto(target_url, wait_until="domcontentloaded", timeout=15000)
        page.wait_for_selector("section.note-item", timeout=20000)
    except Exception as e:
        print("âš ï¸ æœç´¢å¼‚å¸¸:", e)
        input("è¯·æ‰‹åŠ¨å¤„ç†éªŒè¯ç åå›è½¦ç»§ç»­ >>> ")

    results = []
    note_items = page.query_selector_all("section.note-item")[:MAX_NOTES_PER_KEYWORD]
    for item in note_items:
        try:
            if item.query_selector("a.cover") is None:
                continue
            title_elem = item.query_selector("a.title span")
            title = title_elem.inner_text().strip() if title_elem else "N/A"
            author_elem = item.query_selector("a.author .name")
            user_url = item.query_selector("a.author").get_attribute("href")
            user_id = extract_id_from_url(user_url)
            author = author_elem.inner_text().strip() if author_elem else "N/A"
            time_elem = item.query_selector("a.author .time")
            post_time = parse_xiaohongshu_time(time_elem.inner_text().strip()) if time_elem else "N/A"
            href = item.query_selector("a.cover").get_attribute("href")
            detail_url = f"https://www.xiaohongshu.com{href}" if href.startswith("/") else href
            note_id = extract_id_from_url(href)
            results.append({
                "title": title, "author": author, "time": post_time,
                "url": detail_url, "note_id": note_id, "user_id": user_id, "user_url": user_url, 
                "keyword": keyword
            })
            if user_id and not user_exists(user_id, user_url):
                scrape_user_detail(context, user_url)
        except Exception as e:
            print("âŒ ç¬”è®°è§£æå¤±è´¥",e)
            continue
    return results
@with_retry()
def scrape_comments_by_url(context, url):
    print(f"ğŸ§­ æ‰“å¼€ç¬”è®°: {url}")
    page = context.new_page()
    comments = []
    node_text = ""
    try:
        page.add_init_script("""
        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
        window.navigator.chrome = { runtime: {} };
        Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh'] });
        Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
        """)
        page.goto(url, wait_until="domcontentloaded", timeout=20000)
        page.wait_for_selector(".note-scroller", timeout=15000)
        node_text = page.query_selector(".note-text").inner_html()
        attempt = 0
        while True:
            try:
                page.evaluate("document.querySelector('.note-scroller').scrollTo(0, document.querySelector('.note-scroller').scrollHeight)")
                time.sleep(SCROLL_PAUSE + random.random() * 1.5)
            except Exception as e:
                print(f"âš ï¸ æ»šåŠ¨ç¬¬ {attempt+1} æ¬¡å¤±è´¥: {e}")
                time.sleep(2)
            if page.locator('.end-container').count() or page.locator(".no-comments").count():
                break
        if page.locator(".no-comments").count():
            return node_text,[]
        elems = page.query_selector_all(".comment-item")
        for el in elems:
            user_elem = el.query_selector("a.name")
            href = user_elem.get_attribute("href") if user_elem else None
            user_url = f"https://www.xiaohongshu.com{href}" if href and href.startswith("/") else None
            user_id = extract_id_from_url(href)
            content_elem = el.query_selector(".content ")
            time_elem = el.query_selector(".date span:not(.location)")
            location_elem = el.query_selector(".date .location")
            time_str = parse_xiaohongshu_time(time_elem.inner_text().strip()) if time_elem else ""
            comments.append({
                "user": user_elem.inner_text().strip() if user_elem else "åŒ¿å",
                "content": content_elem.inner_text().strip() if content_elem else "",
                "location": location_elem.inner_text().strip() if location_elem else "",
                "time": time_str,
                "user_id": user_id,
                "user_url": user_url
            })
            if user_id and not user_exists(user_id, user_url):
                scrape_user_detail(context, href)
    except Exception as e:
        print("âŒ è¯„è®ºæŠ“å–å¤±è´¥:", e)
    finally:
        page.close()
    return node_text,comments

# ---------- è¾…åŠ©ï¼šå®‰å…¨çš„æŸ¥è¯¢å™¨ ----------
def safe_query_selector(page, selector: str, wait_timeout: int = 2500, retries: int = 2):
    """
    å°è¯•ç­‰å¾…å¹¶æŸ¥è¯¢å…ƒç´ ï¼Œè‡ªåŠ¨æ•è·å› å¯¼èˆªå¯¼è‡´çš„æ‰§è¡Œä¸Šä¸‹æ–‡é”€æ¯é”™è¯¯ã€‚
    è¿”å› page.query_selector(...) æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰ã€‚
    wait_timeout: ç­‰å¾…é€‰æ‹©å™¨çš„è¶…æ—¶(ms)
    retries: å‡ºé”™åçš„é‡è¯•æ¬¡æ•°
    """
    for attempt in range(retries + 1):
        try:
            # å…ˆå°è¯•ç­‰å¾…å…ƒç´ å‡ºç°ï¼ˆçŸ­è¶…æ—¶ï¼‰ï¼Œæœ‰æ—¶ wait_for_selector ä¼šåœ¨å¯¼èˆªä¸­å¤±è´¥ -> æ•è·
            page.wait_for_selector(selector, timeout=wait_timeout)
            return page.query_selector(selector)
        except Exception:
            # å¯èƒ½å› ä¸ºå¯¼èˆªã€é‡æ¸²æŸ“æˆ–æ‰§è¡Œä¸Šä¸‹æ–‡è¢«æ›¿æ¢å¯¼è‡´å¤±è´¥ï¼ŒçŸ­æš‚ sleep åé‡è¯•
            time.sleep(0.3)
            try:
                # æœ€åå†å°è¯•ç›´æ¥ queryï¼ˆè‹¥ä¸Šä¸‹æ–‡ç¨³å®šåˆ™å¯èƒ½æˆåŠŸï¼‰
                return page.query_selector(selector)
            except Exception:
                time.sleep(0.2)
                continue
    return None

def safe_query_selector_text(page, selector: str, wait_timeout: int = 2500, default: str = "") -> str:
    el = safe_query_selector(page, selector, wait_timeout=wait_timeout)
    try:
        return el.inner_text().strip() if el else default
    except Exception:
        return default

# ---------- åœ¨ main() ä¸­ä½¿ç”¨æ›´ç¨³å¥çš„ç™»å½•æ£€æµ‹ ----------
def main():
    init_db()
    with sync_playwright() as p:
        context = p.chromium.launch_persistent_context(
            user_data_dir="./xhs_profile",
            headless=False,
            viewport={"width": 1280, "height": 800},
            args=[
                "--no-sandbox",
                "--disable-blink-features=AutomationControlled",  # å»é™¤è‡ªåŠ¨åŒ–ç‰¹å¾
                "--disable-infobars",
                "--start-maximized",
                "--disable-dev-shm-usage",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process",
                "--window-position=0,0",
            ],
            ignore_https_errors=True,
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/123.0.0.0 Safari/537.36"
            ),
        )


        # è·å–æˆ–åˆ›å»ºç¬¬ä¸€é¡µ
        page = context.pages[0] if context.pages else context.new_page()

        # æ›´ç¨³å¥åœ°æ‰“å¼€é¦–é¡µå¹¶ç­‰å¾…åŸºæœ¬è½½å…¥ï¼ˆç­‰å¾… load çŠ¶æ€ï¼‰
        try:
            page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            window.navigator.chrome = { runtime: {} };
            Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh'] });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
            """)

            page.goto("https://www.xiaohongshu.com", timeout=30000)
            page.wait_for_load_state("domcontentloaded", timeout=15000)
        except Exception as e:
            print("âš ï¸ è®¿é—®å°çº¢ä¹¦é¦–é¡µæ—¶å‘ç”Ÿå¼‚å¸¸ï¼ˆä½†ç¨‹åºç»§ç»­ï¼‰ï¼š", e)

        # ä½¿ç”¨ safe_query_selector æ£€æŸ¥ç™»å½•çŠ¶æ€ï¼ˆé¿å… Execution context é”™è¯¯ï¼‰
        try:
            avatar_el = safe_query_selector(page, "div.user-avatar", wait_timeout=2000)
            name_el = safe_query_selector(page, "span.name", wait_timeout=2000)
            if not avatar_el and not name_el:
                print("âš ï¸ è¯·æ‰‹åŠ¨æ‰«ç ç™»å½•å°çº¢ä¹¦...")
                # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•ï¼šè¿™é‡Œä¹Ÿç”¨ loop æ£€æµ‹ç™»å½•æˆåŠŸï¼Œé¿å… query_selector å¯¼è‡´ error
                while True:
                    input("æ‰«ç ç™»å½•å®ŒæˆåæŒ‰å›è½¦ç»§ç»­ >>> ")
                    # é‡æ–°åŠ è½½å½“å‰é¡µé¢çš„ DOM å¹¶æ£€æµ‹
                    try:
                        page.add_init_script("""
                        Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                        window.navigator.chrome = { runtime: {} };
                        Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh'] });
                        Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
                        """)
                        page.goto("https://www.xiaohongshu.com", timeout=20000)
                        page.wait_for_load_state("domcontentloaded", timeout=10000)
                    except Exception:
                        pass
                    avatar_el = safe_query_selector(page, "div.user-avatar", wait_timeout=2000)
                    name_el = safe_query_selector(page, "span.name", wait_timeout=2000)
                    if avatar_el or name_el:
                        print("âœ… å·²æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€ï¼Œç»§ç»­çˆ¬å–")
                        break
                    else:
                        print("ä»æœªæ£€æµ‹åˆ°ç™»å½•ï¼Œè¯·ç¡®è®¤å·²å®Œæˆæ‰«ç å¹¶åˆ·æ–°é¡µé¢åé‡è¯•ã€‚")
            else:
                print("âœ… å·²æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
        except Exception as e:
            # å…œåº•ï¼šå¦‚æœæ£€æµ‹è¿‡ç¨‹ä»ç„¶æŠ›å‡ºäº†å¼‚å¸¸ï¼Œæ‰“å°å¹¶ç»§ç»­ï¼Œè®©åç»­é€»è¾‘æ›´å®‰å…¨
            print("âš ï¸ ç™»å½•æ£€æµ‹è¿‡ç¨‹ä¸­æ•è·å¼‚å¸¸ï¼ˆç»§ç»­æ‰§è¡Œï¼‰:", e)

        # ä¸»å¾ªç¯ï¼šçˆ¬å…³é”®è¯
        for keyword in KEYWORDS:
            try:
                notes = scrape_keyword(context,page, keyword)
            except Exception as e:
                print(f"âš ï¸ scrape_keyword å‡ºé”™ï¼ˆè·³è¿‡è¯¥å…³é”®è¯ï¼‰ï¼š{keyword} -> {e}")
                continue

            for i, note in enumerate(notes, 1):
                if i % BATCH_SLEEP_AFTER == 0:
                    print(f"ğŸ˜´ è¾¾åˆ° {BATCH_SLEEP_AFTER} ç¯‡ï¼Œä¼‘æ¯ {BATCH_SLEEP_SEC} ç§’ä»¥é˜²é£æ§...")
                    time.sleep(BATCH_SLEEP_SEC)
                try:
                    node_text,comments = scrape_comments_by_url(context, note["url"])
                    note["node_text"] = node_text
                    save_note_to_db(note)
                    if comments:
                        save_comments_to_db(note["url"], comments)
                except Exception as e:
                    print(f"âš ï¸ å¤„ç†ç¬”è®° {note.get('url')} æ—¶å‡ºé”™ï¼š", e)

        context.close()

if __name__ == "__main__":
    main()
